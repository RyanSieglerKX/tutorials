{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec3a307-a161-4d03-81b6-12388f7cbe3d",
   "metadata": {},
   "source": [
    "# Time Series & Historical Query Analysis\n",
    "\n",
    "Welcome to this example where we'll demonstrate how to work with large datasets in kdb+ to analyze time-series data. \n",
    "\n",
    "One of the key features of kdb+ is its ability to handle huge volumes of data with exceptional speed and efficiency. Whether it's reading massive datasets, performing time-based aggregations, or joining data from different sources, kdb+ excels at time-series analysis. By the end of this example, you'll have a clear understanding of how to create, manipulate, store, and analyze data using kdb+/q. Along the way, we'll introduce several key concepts that are fundamental to working with kdb+/q.\n",
    "\n",
    "\n",
    "Here, we'll cover:\n",
    "- Creating a large time-series dataset from scratch\n",
    "- Saving this data to a database on disk\n",
    "- Scaling database to 1 Billion rows\n",
    "- Performing time-based aggregations to analyze trends over time\n",
    "- Using asof joins (aj) to combine time-series data (e.g., matching trades to quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c6c31-ab9a-4349-b0f9-5c541553f284",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "1. For setup instructions and prerequisites, please refer to the [README](README.md).\n",
    "2. Ensure PyKX is properly initialized by running the cell below.<br/>\n",
    "   <b>Note</b>: This is a Python cell that will enable the kernel to execute q code as the default language for all later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76de5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKX now running in 'jupyter_qfirst' mode. All cells by default will be run as q code. \n",
      "Include '%%py' at the beginning of each cell to run as python code. \n"
     ]
    }
   ],
   "source": [
    "import pykx as kx\n",
    "kx.util.jupyter_qfirst_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203051bc-25ce-441f-ac35-e078d2499a38",
   "metadata": {},
   "source": [
    "## 2. Create the Time Series Dataset\n",
    "\n",
    "Letâ€™s start by creating a sample dataset to work with. This dataset will simulate trade data over a period of time, with random values for price, size, and symbols. Weâ€™ll generate 20 million rows of trade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b63cdf-8139-4b53-810f-3084fdb1b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:20000000\n",
    "syms:100?`$read0 `:stocks.txt               // Select 100 random symbols from external file\n",
    "day:2025.01.01\n",
    "trade:([] \n",
    "    time:asc (`timestamp$day) + n?24:00:00;     // Start from midnight, spread across 24h\n",
    "    sym:n?syms;                                 // Random stock tickers\n",
    "    price:n?100f;                               // Random trade prices\n",
    "    size:n?1000                                 // Random trade sizes\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691b30-afb5-4f3b-b9d5-680537770eb9",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening:\n",
    "- `n: 2000000` sets the number of rows we want to generate\n",
    "- `syms` Selects 100 random stock symbols from an external text file containing a list of real stock tickers.\n",
    "- We define a new table with table notation `([] col1:<values>; col2:<values>: ...)`\n",
    "- We use `?` to generate random values for 4 columns:\n",
    "    - `time` is populated with timestamps starting from midnight and increasing across a 24-hour period, with a random offset to simulate a spread of trades.\n",
    "    - `sym` is populated with random symbols, selected from a list.\n",
    "    - `price` and trade `size` are randomnly generated\n",
    "\n",
    "This table is now available in memory to investigate and query. Let's take a quick look at the row [`count`](#https://code.kx.com/q/ref/count/), schema details with [`meta`](#https://code.kx.com/q/ref/meta/) and first 10 rows using [`sublist`](#https://code.kx.com/q/ref/sublist/).\n",
    "\n",
    "These simple commands are essential when exploring your data quickly in kdb+/q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfb0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000\n"
     ]
    }
   ],
   "source": [
    "count trade              // get row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748c29a6-ff5d-477f-ac9b-18ff47fc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "time | p   s\n",
      "sym  | s    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade               // get table schema details - datatypes, column names etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d60c8",
   "metadata": {},
   "source": [
    "The following columns are produced when we run `meta`:\n",
    "- c: column name\n",
    "- t: <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">column type</a>\n",
    "- f: <a href=\"https://code.kx.com/q4m3/8_Tables/#85-foreign-keys-and-virtual-columns\" target=\"_blank\">foreign keys</a>\n",
    "- a: <a href=\"https://code.kx.com/q/ref/set-attribute/\" target=\"_blank\">attributes</a> (modifiers applied for performance optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1a6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                          sym   price    size\n",
      "-------------------------------------------------\n",
      "2025.01.01D00:00:00.000000000 MFICL 64.20376 597 \n",
      "2025.01.01D00:00:00.000000000 TEAM  30.63798 172 \n",
      "2025.01.01D00:00:00.000000000 RVYL  40.56048 879 \n",
      "2025.01.01D00:00:00.000000000 SIGI  57.2691  829 \n",
      "2025.01.01D00:00:00.000000000 DVSP  54.74414 658 \n",
      "2025.01.01D00:00:00.000000000 HYDR  61.67117 925 \n",
      "2025.01.01D00:00:00.000000000 ELAB  6.223127 784 \n",
      "2025.01.01D00:00:00.000000000 HYLS  75.65475 755 \n",
      "2025.01.01D00:00:00.000000000 WGMI  78.49312 596 \n",
      "2025.01.01D00:00:00.000000000 NRES  40.66333 747 \n"
     ]
    }
   ],
   "source": [
    "10 sublist trade         // get first 10 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85659a55-7044-49c8-8e70-4fcd7c92674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used| 740288096\n",
      "heap| 3221225472\n",
      "peak| 3221225472\n",
      "wmax| 0\n",
      "mmap| 640008056\n",
      "mphy| 67436511232\n",
      "syms| 8720\n",
      "symw| 401941\n"
     ]
    }
   ],
   "source": [
    ".Q.w[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2a1d-2ca1-4235-a6d9-28779a361fdd",
   "metadata": {},
   "source": [
    "## 3.  Save Data to Disk\n",
    "\n",
    "Once the data is generated, youâ€™ll likely want to save it to disk for persistent storage.\n",
    "\n",
    "Because we want the ability to scale, partitioning by date will be a good approach for this dataset. Without partitioning, queries that span large time periods would require scanning entire datasets, which can be very slow and resource-intensive. By partitioning data, kdb+ can limit the query scope to the relevant partitions, significantly speeding up the process.\n",
    "\n",
    "To partition by date we can use the inbuilt function [`.Q.dpft`](#https://code.kx.com/q/ref/dotq/#dpft-save-table) to save the data to disk - this may take ~20 seconds to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e9616f-87f5-4456-99d2-da888625bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "homeDir:getenv[`HOME]                   // Get the home directory for edu.kx.com\n",
    "dbDir:homeDir,\"/data\"                   // Define database location as string\n",
    "dbPath:hsym `$dbDir                     // Database location as hsym for file I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c36306-d885-41dd-a292-8b24a9f7a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    ".z.zd:(17;2;6)                          // Set compression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a070de1a-2796-47ca-935b-d2fd38d0ae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade\n"
     ]
    }
   ],
   "source": [
    ".Q.dpft[dbPath;day;`sym;`trade]         // Save data as a partitioned database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1914853",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- <a href=\"https://code.kx.com/q/ref/hsym/\" target=\"_blank\">hsym</a>: This function prefixes the directory location with a colon to make it a file handle\n",
    "- <a href=\"https://code.kx.com/q/ref/dotz/#zzd-compressionencryption-defaults\" target=\"_blank\">.z.d</a>: This function sets the compression parameters.\n",
    "- <a href=\"https://code.kx.com/q/ref/dotq/#dpft-save-table\" target=\"_blank\">.Q.dpft[d;p;f;t]</a>: This command saves data to a <b>(d)</b>atabase location, targeting a particular <b>(p)</b>artition and indexes the data on a chosen <b>(f)</b>ield for the specified <b>(t)</b>able.\n",
    "\n",
    "One persisted, the table name is returned. We can test its worked as expected by deleting the `trade` table we have in memory and reloading the database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "236cb599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                     // Delete in memory table\n",
    "system\"l \",dbDir                         // Load the partitioned database\n",
    "meta trade                               // Check it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bdda4",
   "metadata": {},
   "source": [
    "kdb+ actually offers a number of different methods to store tables which will allow for efficient storage and querying for different sized datasets: flat, splayed, partitioned and segmented.\n",
    "\n",
    "A general rule of thumb around which format to choose depends on three things:\n",
    "\n",
    "- Will the table continue to grow at a fast rate?\n",
    "- Am I working in a RAM/memory constrained environment?\n",
    "- What level of performance do I want?\n",
    "\n",
    "To learn more about these types and when to choose which <a href=\"https://code.kx.com/q/database/\" target=\"_blank\">see here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb2d99-5516-4bd7-8c37-bd58a6c0cd35",
   "metadata": {},
   "source": [
    "## 4 Scaling Dataset to 1 Billion Rows\n",
    "\n",
    "In this section, we scale our dataset to 1 billion rows by duplicating an existing partition across multiple days. This approach ensures we have sufficient data for performance testing and analytics validation.\n",
    "\n",
    "Before making copies, we check the disk space usage to ensure enough storage is available. The below system command displays the available and used disk space in megabytes (~9.6G), helping us monitor the impact of our operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9eb5f8-2106-4dc1-930f-4ba7f38a3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdc        9.8G  183M  9.6G   2% /home/jovyan\"\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ad551-c666-4120-849a-8940ad34929e",
   "metadata": {},
   "source": [
    "Next, we generate a list of new dates and copy the existing partition (2025.01.01) to these new dates. Here, we create 49 new partitions by copying the original partition for each additional day. This may take ~40 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b06d4b8-a8c5-42be-b9c6-7f3241b017f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55011\n"
     ]
    }
   ],
   "source": [
    "days:day +1 +til 49;                                         // Generate 49 additional days  \n",
    "cmds: \"cp -r ../data/2025.01.01 ../data/\",/:string[days];    // Create shell commands to execute\n",
    "\\t system each cmds                                          // Execute shell commands to copy partitions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e8193-91ac-4eef-9db6-bb15513ca9be",
   "metadata": {},
   "source": [
    "Once the partitions are created, we verify how much disk space was consumed and check the new partitions exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0497fc63-2117-4653-894a-3790b4ae5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Filesystem      Size  Used Avail Use% Mounted on\"\n",
      "\"/dev/sdc        9.8G  8.9G  893M  92% /home/jovyan\"\n",
      "\"total 216\"\n",
      "\"drwxrwsr-x 53 jovyan users 4096 Mar  7 12:18 .\"\n",
      "\"drwxrwsr-x 10 root   users 4096 Mar  7 12:11 ..\"\n",
      "\"drwxr-sr-x  2 jovyan users 4096 Mar  7 12:05 .ipynb_checkpoints\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.01\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.02\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.03\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.04\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.05\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.06\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.07\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.08\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.09\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.10\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.11\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.12\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.13\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.14\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:17 2025.01.15\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:18 2025.01.16\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:18 2025.01.17\"\n",
      "\"drwxr-sr-x  3 jovyan users 4096 Mar  7 12:18 2025.01.18\"\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "system\"df -mh .\"\n",
    "system\"ls -la ../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e21679-4e18-4982-8a4a-dc164db658fe",
   "metadata": {},
   "source": [
    "Finially since kdb+ manages partitioned data at the filesystem level, we must reload the database to reflect the newly added partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88481f47-c923-410f-ae2f-6b6741686c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "1000000000\n",
      "date      | x       \n",
      "----------| --------\n",
      "2025.01.01| 20000000\n",
      "2025.01.02| 20000000\n",
      "2025.01.03| 20000000\n",
      "2025.01.04| 20000000\n",
      "2025.01.05| 20000000\n",
      "2025.01.06| 20000000\n",
      "2025.01.07| 20000000\n",
      "2025.01.08| 20000000\n",
      "2025.01.09| 20000000\n",
      "2025.01.10| 20000000\n",
      "2025.01.11| 20000000\n",
      "2025.01.12| 20000000\n",
      "2025.01.13| 20000000\n",
      "2025.01.14| 20000000\n",
      "2025.01.15| 20000000\n",
      "2025.01.16| 20000000\n",
      "2025.01.17| 20000000\n",
      "2025.01.18| 20000000\n",
      "2025.01.19| 20000000\n",
      "2025.01.20| 20000000\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "delete trade from `.                               // Delete in memory table\n",
    "system\"l \",dbDir                                   // Load the partitioned database\n",
    "count trade                                        // 1B Rows \n",
    "select count i by date from trade                  // Select number of records per date within the trade table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ac59",
   "metadata": {},
   "source": [
    "## 5. Time Series Analytics\n",
    "\n",
    "Now that we have 1 Billion rows of data, let's dive into some basic time-series analytics.\n",
    "\n",
    "### Total Trade Volume Every Hour\n",
    "\n",
    "Let's find a symbol to analyse from the randomly generated list we created earlier and then run our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd0f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| size   \n",
      "-----------------| -------\n",
      "2025.01.01 00:00 | 4168236\n",
      "2025.01.01 01:00 | 4160249\n",
      "2025.01.01 02:00 | 4186595\n",
      "2025.01.01 03:00 | 4187285\n",
      "2025.01.01 04:00 | 4180584\n",
      "2025.01.01 05:00 | 4113079\n",
      "2025.01.01 06:00 | 4205680\n",
      "2025.01.01 07:00 | 4101998\n",
      "2025.01.01 08:00 | 4167611\n",
      "2025.01.01 09:00 | 4108081\n",
      "2025.01.01 10:00 | 4138310\n",
      "2025.01.01 11:00 | 4178508\n",
      "2025.01.01 12:00 | 4134138\n",
      "2025.01.01 13:00 | 4161473\n",
      "2025.01.01 14:00 | 4128168\n",
      "2025.01.01 15:00 | 4204660\n",
      "2025.01.01 16:00 | 4221971\n",
      "2025.01.01 17:00 | 4186433\n",
      "2025.01.01 18:00 | 4203052\n",
      "2025.01.01 19:00 | 4155269\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "symbol:first syms\n",
    "select sum size \n",
    "    by date,\n",
    "       60 xbar time.minute \n",
    "    from trade \n",
    "    where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944524",
   "metadata": {},
   "source": [
    "#### qSQL & Temporal Arithmetic\n",
    "Here we are using <a href=\"https://code.kx.com/q/basics/qsql/\" target=\"_blank\">qSQL</a>, the inbuilt table query language in kdb+. If you have used SQL, you will find the syntax of qSQL queries very similar.\n",
    "- Just as in SQL, table results called using `select` and `from` and can be filtered by expressions following a `where`\n",
    "- Multiple filter criteria, separated by ,, are evaluated starting from the left\n",
    "- To group similar values together we can use the `by` clause. This is particularly useful in combination with used with an aggregation like `sum`,`max`,`min` etc.\n",
    "\n",
    "kdb+/q supports several temporal types and arithmetic between them. See here for a summary of <a href=\"https://code.kx.com/q/ref/#datatypes\" target=\"_blank\">datatypes</a>.\n",
    "\n",
    "In this example:\n",
    "- The `time` column in the data has a type of timestamp, which includes both date and time values.\n",
    "- We convert the `time` values to their minute values (including hours and minutes)\n",
    "- We then aggregate further on time by using <a href=\"https://code.kx.com/q/ref/xbar/\" target=\"_blank\">xbar</a> to bucket the minutes into hours (60-unit buckets)\n",
    "\n",
    "### Weighted Average Price and Last Trade Price Every 15 Minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1172a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       minute| lastPx   vwapPx  \n",
      "-----------------| -----------------\n",
      "2025.01.01 00:00 | 12.02315 49.7027 \n",
      "2025.01.01 00:15 | 89.32436 50.23902\n",
      "2025.01.01 00:30 | 69.63196 49.84172\n",
      "2025.01.01 00:45 | 45.60034 49.13936\n",
      "2025.01.01 01:00 | 76.59549 49.59122\n",
      "2025.01.01 01:15 | 72.53248 51.27943\n",
      "2025.01.01 01:30 | 6.074879 49.90891\n",
      "2025.01.01 01:45 | 64.48105 50.05766\n",
      "2025.01.01 02:00 | 34.01241 49.25463\n",
      "2025.01.01 02:15 | 80.39318 50.41682\n",
      "2025.01.01 02:30 | 57.6013  49.74883\n",
      "2025.01.01 02:45 | 35.34707 48.33145\n",
      "2025.01.01 03:00 | 62.20132 50.13491\n",
      "2025.01.01 03:15 | 55.43684 48.93589\n",
      "2025.01.01 03:30 | 71.24645 49.03167\n",
      "2025.01.01 03:45 | 41.56698 50.04032\n",
      "2025.01.01 04:00 | 21.19538 49.90231\n",
      "2025.01.01 04:15 | 24.41028 49.78817\n",
      "2025.01.01 04:30 | 41.9978  49.86906\n",
      "2025.01.01 04:45 | 32.51881 49.48584\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    " by date, 15 xbar time.minute \n",
    " from trade \n",
    " where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc755377",
   "metadata": {},
   "source": [
    "This is similar to the previous analytic, but this time we make use of the built in `wavg` function to find out the weighted average over time intervals. \n",
    "\n",
    "In finance, volume-weighted averages give a more accurate reflection of a stockâ€™s price movement by incorporating trading volume at different price levels. This can be especially useful in understanding whether a price move is supported by strong market participation or is just a result of a few trades.\n",
    "\n",
    "Let's time this anayltic with `\\t` to see how long it takes in milliseconds to crunch through 1 Billion records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d040fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "\\t select lastPx:last price, \n",
    "       vwapPx:size wavg price\n",
    "   by date, 15 xbar time.minute \n",
    "   from trade \n",
    "   where sym=symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea65524",
   "metadata": {},
   "source": [
    "The query processed 1 Billion records in sub second time, efficiently aggregating last price (`lastPx`) and volume-weighted-average price (`vwapPx`) for these trades. The use of `by date, 15 xbar time.minute` optimized the grouping, making the computation fast. This demonstrates the power of kdb+/q for high-speed time-series analytics.\n",
    "\n",
    " ### SQL Comparison\n",
    "\n",
    "A SQL version of this query above would look something like:\n",
    "\n",
    "```\n",
    "\n",
    "SELECT \n",
    "    (array_agg(price ORDER BY time DESC))[1] AS lastPx,\n",
    "    SUM(price * size) / NULLIF(SUM(size), 0) AS vwapPx,\n",
    "    DATE_TRUNC('day', time),                                            \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE) \n",
    "FROM \n",
    "    trade\n",
    "WHERE \n",
    "    sym = 'MSFT'\n",
    "GROUP BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE)\n",
    "ORDER BY \n",
    "    DATE_TRUNC('day', time), \n",
    "    TRUNC(time, 'MI') + (FLOOR(TO_NUMBER(TO_CHAR(time, 'MI')) / 15) * INTERVAL '15' MINUTE);\n",
    "\n",
    "```\n",
    "\n",
    "SQL is more complex due to several factors:\n",
    "- **Time-series Calculations**: The SQL version involves the creation of custom logic for common time-series calculations such as volume-weighted-averages. In the q-sql version, these functionalities are implicit, and the syntax is more concise when working with vectors. The SQL equivalent requires custom definitions and is often more verbose leaving room for error.\n",
    "- **Grouping and Aggregation**: In the q-sql version, grouping by date and a 15 minute window is done with a single, simple syntax, which is an efficient and intuitive way to express time bucketing. In SQL, similar behavior requires explicitly defining how time intervals are handled and aggregating the results using GROUP BY with custom time expressions which are often repeated throughout the query.\n",
    "- **Temporal Formatting**: SQL queries often require repetitive conversion for handling timestamp formats, which is more cumbersome compared to q-sql, where time-based operations like xbar (interval-based bucketing) can be done directly in a streamlined manner. Temporal primitives also make it extremely easy to convert a nanosecond timestamp to it's equivalent minute using dot notation e.g. time.minute\n",
    "- **Data Transformation**: The q language is optimized for high-performance, in-memory, columnar data transformations, which allows for more compact expressions on vectors of data. SQL, on the other hand, is typically too general purpose for even simple transformations on time-series data. This is down to how kdb+/q is designed, where operations execute on ordered lists, whereas SQL (based on set theory) treats data as records instead of columns e.g. selecting the (last) value in a series, or understanding prior states (deltas) for series movements would require re-ordering the column data\n",
    "- **Performance Considerations**: q-sql is designed for high-performance analytics on large datasets, and many operations that would require complex SQL expressions can be done efficiently with q-sql syntax. In SQL, complex operations requires workarounds such as additional processing with temporary tables, sub-expressions, re-indexing, changing data models, or heavily leveraging partitions and window functions.\n",
    "\n",
    "Thus, while the core logic of the query is similar in both languages, the SQL version requires much more overhead in terms of complexity and verbosity. This inefficiency will also become more pronounced with large datasets, leading to challenges with query performance.\n",
    "\n",
    "While these are just basic analytics, they highlight kdb+/qâ€™s ability to storage and analyse large-scale time-series datasets quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b806e",
   "metadata": {},
   "source": [
    "## 6. Asof Join â€“ Matching Trades with Quotes\n",
    "\n",
    "One of the most powerful features in kdb+/q is the asof join (`aj`), which is designed to match records from two tables based on the most recent timestamp. Unlike a standard SQL join, where records must match exactly on a key, an asof join finds the most recent match.\n",
    "\n",
    "Why Use Asof Joins?\n",
    "In time-series data, we often deal with information arriving at different intervals. For example:\n",
    "- Trade and Quote Data: A trade occurs at a given time, and we want to match it with the latest available quote.\n",
    "- Sensor Data: A sensor records temperature every second, while another logs environmental data every 10 secondsâ€”matching the closest reading is crucial.\n",
    "\n",
    "> **ðŸ“Œ** kdb+/q optimizes asof joins to handle large datasets efficiently, making it a key tool in real-time analytics and historical data analysis.\n",
    "\n",
    "#### Generate synthetic quote data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44e0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n:2000000\n",
    "quote:([] \n",
    "    time:asc (`timestamp$day) + n?86400000000000;  // Random timestamps\n",
    "    sym:n?syms;                                    // Random stock tickers\n",
    "    bid:n?100f;                                    // Random bid prices\n",
    "    ask:n?100f                                     // Random ask prices\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0b9f",
   "metadata": {},
   "source": [
    "As we're keeping this table in memory we need to perform one extra step before joining, we apply the parted (p#) attribute to the sym column of the quote table. Our trade table on disk already has the parted attribute on the sym column, we see this in the column `a` when we run `meta trade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4c9937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c    | t f a\n",
      "-----| -----\n",
      "date | d    \n",
      "sym  | s   p\n",
      "time | p    \n",
      "price| f    \n",
      "size | j    \n"
     ]
    }
   ],
   "source": [
    "meta trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781da43",
   "metadata": {},
   "source": [
    "This is crucial for optimizing asof joins, as it ensures faster lookups when performing symbol-based joins. Before applying parted to quote, we first sort the table by sym using [`xasc`](#https://code.kx.com/q/ref/asc/), as the parted attribute requires the column to be sorted for it to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "458e1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote:`sym xasc quote                  / Sorting sym in ascending order\n",
    "quote:update `p#sym from quote         / Apply the parted attruibute on sym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fd87b",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `xasc` Sorts the quote table by sym in ascending order\n",
    "- `#`  Applies the parted attribute to sym, optimizing symbol-based lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7a1a4",
   "metadata": {},
   "source": [
    "#### Peform Asof Join\n",
    "\n",
    "We now match each trade with the most recent available quote for todays date using [`aj`](#https://code.kx.com/q/ref/aj/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0bb0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       sym  time                          price    size bid      ask     \n",
      "-----------------------------------------------------------------------------\n",
      "2025.01.01 AAME 2025.01.01D00:00:00.000000000 11.13743 579                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:01.000000000 25.39669 530                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:02.000000000 52.84274 139                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:03.000000000 29.17217 227                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:03.000000000 95.41841 735                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:04.000000000 60.95445 995                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:04.000000000 63.20168 324                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:04.000000000 78.9044  684                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:04.000000000 18.60533 47                    \n",
      "2025.01.01 AAME 2025.01.01D00:00:05.000000000 41.88837 882                   \n",
      "2025.01.01 AAME 2025.01.01D00:00:06.000000000 17.04109 901  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:07.000000000 38.12558 377  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:07.000000000 2.801231 474  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:08.000000000 90.15238 745  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:08.000000000 62.14185 393  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:09.000000000 65.49808 42   10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:09.000000000 3.75946  778  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:09.000000000 41.3384  901  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:09.000000000 35.3266  859  10.01786 8.147825\n",
      "2025.01.01 AAME 2025.01.01D00:00:10.000000000 58.99907 507  10.01786 8.147825\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "tradequote:aj[`sym`time; select from trade where date=day; quote]\n",
    "tradequote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd376e1",
   "metadata": {},
   "source": [
    "In the above:\n",
    "- `aj` performs an asof join on the `sym` and `time` columns\n",
    "- Each trade record gets matched with the latest available quote at or before the tradeâ€™s timestamp.\n",
    "- We can see this means the first few `bid` and `ask` values are empty because there was no quote data prior to those trades.\n",
    "\n",
    "This approach ensures that for every trade, we have the best available quote information, allowing traders to analyze trade execution relative to the prevailing bid/ask spread at the time.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try [Example2](Example2.html) on Real-Time Ingestion & Streaming Analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
