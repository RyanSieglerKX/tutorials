# IVF with Partitioning on Wikipedia Embeddings

IVF (Inverted File Index) is an ANN (Approximate Nearest Neighbour) search method that splits data into clusters using K-Means clustering to reduce the search space and speed up searches. It works best if your data naturally falls into distinct clusters (like a news dataset could have distinct categories of sports, weather and politics)

This tutorial walks through the process of clustering data using the ai-libs and creating a historical database (HDB), partitioned on `int`, filled with wikipedia simplified articles. It then demonstrates how to conduct a manual IVF search, utilising the ai-libs' `flat` functionality.

## 1. Prerequisites

1. Requires KDB-X to be installed, you can sign up at https://developer.kx.com/products/kdb-x/install.
2. Ensure you have the necessary dataset:
   1. Download the dataset from Kaggle: [Wikipedia Embeddings](https://www.kaggle.com/datasets/stephanst/wikipedia-simple-openai-embeddings?resource=download)
   2. Extract `archive.zip` into the same directory where you are running this from. The file `requests_for_openai_embeddings_result.jsonl` should be available.

## 2. Loading and Preparing the Data

Launch a q session.

> Note: To take advantage of parallel execution, you must start your process with secondary threads enabled. You can do this using the -s command-line flag when launching the process e.g. `q -s 4`

Then load the ai-libs initialization script:
```q
\l ai-libs/init.q
```
### Load the dataset

Load the dataset (`requests_for_openai_embeddings_result.jsonl`) into a table:

```q
file:`$":requests_for_openai_embeddings_result.jsonl";
t:([] input:();embedding:());
.Q.fsn[{`t upsert raze {a:.j.k x;:enlist @[(a[0]_`model),flip select embedding from a[1][`data];`embedding;{`real$first x}]}peach x};;30000000] file; // This takes ~90 seconds on 4 threads
```

In the above:
- The function `.Q.fsn` is used to read plain text files too large to fit into memory, it loops over a file in 30000000 byte-sized chunks of complete records, and applies a function to each chunk
- The upsert operation works in place, modifying the original value, by passing `t` by name
- The function `.j.k` deserialises json into a Q object
- The results came from running on 4 secondary threads, exection time will vary depending on your specific secondaries and dataset. You can adjust the number based on your system’s CPU cores.


Now the dataset has been loaded, we can inspect the table using `first` to see what the first row of the table looks like:
```q
first t
```
    input    | "Title: Federal Ministry for Economic Affairs and Climate Action Content: The Federal ..
    embedding| 0.001621442 -0.02399798 -0.02782375 -0.0234312 -0.0326929 0.00378068 0.001189111 -0.01..

The first result from the table shows a single row containing:
- input: A text string (e.g., a Wikipedia excerpt or article title and content).
- embedding: A high-dimensional numeric vector representing the semantic meaning of that text, typically used for similarity search, clustering, or classification.

### Train the clustering model

Now that we understand the structure of the data, we can move on to clustering the data. We split the data into 32 clusters using `.ai.ivf.train` which trains a clustering model (using IVF) on 10,000 randomly sampled embeddings with L2 distance.

```q
nlist:32;
repPts:.ai.ivf.train[nlist;neg[10000]?t`embedding;`L2]
```
    -0.003112307 -0.009494112  0.007978876  -0.01247206 -0.02248432  0.01755531  -0.01333215  -0.0053..
    0.002116795  0.003132874   -0.002279328 -0.01087256 0.004709429  0.01824356  -0.01469742  -0.0159..
    0.007057028  0.0001529294  0.004283461  -0.01290778 -0.007193359 0.03692468  -0.01920409  -0.0112..
    -0.01494405  -0.01013848   0.004345792  -0.01979265 -0.007949634 0.02756895  -0.01265423  -0.0067..
    0.002490363  0.0027059     0.003393865  -0.01398009 -0.005690602 0.021657    -0.008214854 -0.0014..
    -0.005712249 -0.00689968   0.0001041025 -0.02143133 -0.0113848   0.01836004  -0.008928992 -0.0034..
    ..

### Assign data points to clusters and group

`repPts` are the resulting cluster centroids. Next we assign each of our embeddings in the full dataset `t` to the nearest cluster centroid based on L2 distance.

```q
predictions:.ai.ivf.predict[repPts;t`embedding;`L2]
```

We create a variable lookup grouping row indices by their predicted cluster, to allow us to see which vectors belong to each cluster when we write down:

```q
lookup:@[group predictions;til nlist]
```
### Save clustered data to disk

Finally we can save this data to disk:

```q
{[t;lookup;repPts;i]
    (hsym `$"db/",string[i],"/wiki/") set t[lookup[i]];
    (hsym `$"db/",string[i],"/centroid/") set ([]centroid:enlist repPts[i]);
    }[t;lookup;repPts;] peach til nlist;
.Q.chk[`:db];
delete t from `.;
.Q.gc[];
```

In the above:
- `set` saves the wiki and centoid data data to partitioned tables on disk
- `.Q.chk` verifies database integrity
- deleting from __`.__ removes t from the local namespace, allowing it to be garbage collection
- `.Q.gc[]` tells Q to reclaim memory and release back to the OS

Now lets query against our newly created database.

## 3. Performing Manual IVF Searches

First we load the previously saved clustered data from disk.

```q
.Q.lo[`:db;0;0]
```

Next we set a few hyperparameters, a seed so you can follow along, and a query vector:

```q
\S 42     // Set random seed for reproducibility
k:10;     // Number of nearest neighbours to return
nprobe:8; // Number of clusters to search
```
Now, we randomly select a query document which gives us a sample Wikipedia entry:
```q
query:select from wiki where int = rand count int, i = rand count i
```
    int input                                                                                        ..
    -------------------------------------------------------------------------------------------------..
    4   "Title: Arab, Alabama Content: Arab is a city in Marshall County, Alabama. Arab has a populat..

Extract the embedding vector for this query:

```q
q:first query`embedding
```
    -0.003525208 -0.01621845 0.004091982 -0.03246181 -0.02720514 0.01573264 -0.01304203 -0.004063955 ..


Next we create a utility function that we will reuse to perform an IVF search on our partitioned database

```q
.walkthroughUtil.search:{[q;k;nprobe]
    clusts:(select centroid from centroid)`centroid;
    clustToQuery:.ai.flat.search[clusts;q;nprobe;`L2][1];
    res:select from wiki where int in clustToQuery;
    s:exec .ai.flat.search[embedding;q;k;`L2] from res;
    `dist`input xcols (res@s[1]),'([]dist:s[0])
 };
```

In the above we:
- Load all centroids
- Find the top nprobe closest centroids to the query vector with `.ai.flat.search`
- Gathers all wiki entries in those clusters
- Runs flat search on this reduced set
- Returns top k closest entries along with distances

Let's search!

```q
.walkthroughUtil.search[q;k;nprobe]
```
    dist      input                                                                                  ..
    -------------------------------------------------------------------------------------------------..
    0         "Title: Arab, Alabama Content: Arab is a city in Marshall County, Alabama. Arab has a p..
    0.1510194 "Title: Albertville, Alabama Content: Albertville is the largest city in Marshall Count..
    0.2427092 "Title: Marion, Alabama Content: Marion is the county seat of Perry County, Alabama. As..
    0.2559958 "Title: Marshall County, Alabama Content: Marshall County is a county in the U.S. state..
    0.2704675 "Title: Marion County, Alabama Content: Marion County is a county in the U.S. state of ..
    0.2740799 "Title: Hamilton, Alabama Content: Hamilton is a city in Marion County, Alabama, United..
    0.2828742 "Title: Tuscaloosa, Alabama Content: Tuscaloosa is a city and county seat of Tuscaloosa..
    0.2859524 "Title: Perry County, Alabama Content: Perry County is a county in the U.S. state of Al..
    0.2880146 "Title: Monroe County, Alabama Content: Monroe County is a county in the southern part ..
    0.2883845 "Title: Arley, Alabama Content: Arley is a town in the U.S. state of Alabama. Category:..

Our search has returned a lot more cities, counties and towns in the U.S. state of Alabama which is semantically similar to the query.

We can validate performance and accuracy against a standard flat search:

```q
\ts:10 t1:.walkthroughUtil.search[q;k;nprobe]
```
    687 502260336
```q
\ts:10 s:exec .ai.flat.search[embedding;q;k;`L2] from select embedding from wiki; t2: `dist`input xcols ((select from wiki)@s[1]),'([]dist:s[0])
```
    5479 2174132352

```q
t1~t2
```
    1b

Nearly 10x faster using IVF over Flat here, and it has 100% recall @10 neighbours.

IVF (Inverted File Index) enables fast, scalable nearest-neighbour searches by limiting comparisons to a subset of clustered data. This delivers:
- Real-time semantic search performance
- Lower infrastructure costs
- High accuracy at scale

## Conclusion

This tutorial demonstrated how to:
- Load and explore partitioned embedding data
- Define and run a manual IVF search using centroids
- Compare IVF performance against brute-force flat search
- Achieve significant speedup (≈10×) with 100% recall on top-k
