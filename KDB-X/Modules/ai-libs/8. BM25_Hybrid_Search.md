# 8. BM25 & Hybrid Search
​
This tutorial walks through the process of performing a hybrid search, which combines the strengths of both sparse (keyword-based BM25) and dense (vector-based) search methods. By merging the results of these two approaches, we can achieve a higher level of accuracy than either method could provide on its own.
​
We will be using the NanoMSMARCO dataset to:
​
- Ingest data into a KDB-X table.
- Generate embeddings and tokenize the text using KDB-X Python.
- Perform sparse and dense searches.
- Consolidate the results into a hybrid search.
​
## Why Hybrid Search?

Sparse Search (like BM25) is excellent at finding documents with exact keyword matches. It's fast and efficient.

Dense Search (using vector embeddings) excels at finding semantically similar documents, even if they don't share the same keywords. It understands context and meaning.

By combining them, we get the best of both worlds: the precision of keyword matching and the contextual understanding of semantic search.

---
## 1. Prerequisites

Before we begin, ensure you have the following installed:

- KDB-X Community Edition, you can sign up at https://developer.kx.com/products/kdb-x/install.
- Install KDB-X Python and enable under q
    ```python
    pip install --upgrade --pre pykx
    python -c "import pykx as kx; kx.install_into_QHOME()"
    ```
- Install requirements.txt 
    ```python
    pip install --index-url https://download.pytorch.org/whl/cpu torch==2.4.1  #install lighter cpu torch
    pip install sentence-transformers==3.3.1
    pip install datasets
    ```
- Now, start KDB-X with `q`

---

## 2. Using KDB-X Python for Text Embeddings and Tokenization

In this section, we'll use the power of KDB-X Python to load our dataset, create vector embeddings for our text, and tokenize it for our sparse search.

### 2.1 Load the Dataset

First, we load the necessary libraries and suppress any warnings to keep our output clean.
```q
.ai:use`kx.ai
\l pykx.q
.pykx.pyexec"import warnings";
.pykx.pyexec"warnings.simplefilter(\"ignore\")";
```
Now, we'll use the datasets library from Hugging Face to load the [NanoMSMarco](https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO) dataset.
NanoMSMARCO is a compact version of the large-scale MS MARCO passage ranking dataset. It contains a corpus of documents, a set of queries, and relevance labels that map queries to the most relevant documents. Because it is lightweight while still preserving the structure of a real information retrieval benchmark, it is excellent for experimenting with hybrid search methods. 
It allows you to test both sparse (BM25 keyword-based) and dense (embedding-based) approaches, and then evaluate hybrid combinations against ground-truth relevance rankings—all without the heavy compute requirements of the full MS MARCO dataset.

This dataset is split into three parts:

- corpus: The collection of documents we will be searching through.
- queries: A set of search queries.
- relevance: A "ground truth" mapping of which documents are most relevant for each query.

```q
// Load data from Hugging Face
.pykx.pyexec "from datasets import load_dataset";
.pykx.pyexec "corpus_ds = load_dataset('zeta-alpha-ai/NanoMSMARCO', 'corpus', split='train')";
.pykx.pyexec "queries_ds = load_dataset('zeta-alpha-ai/NanoMSMARCO', 'queries', split='train')";
.pykx.pyexec "rankings_ds = load_dataset('zeta-alpha-ai/NanoMSMARCO', 'qrels', split='train')";
```
```q
// Convert Python object to q dictionaries 
corpus:.pykx.qeval "{int(item['_id']): pykx.CharVector(item['text']) for item in corpus_ds}";
queries:.pykx.qeval "{int(item['_id']): pykx.CharVector(item['text']) for item in queries_ds}";
rankings:.pykx.qeval "{int(item['query-id']): int(item['corpus-id']) for item in rankings_ds}";
```
```q
//Create tables with consistent data types
corpus:([]docId:key corpus;text:value corpus);
queries:([]docId:key queries;text:value queries);
rankings:([]docId:key rankings;rankedIds:value rankings);

rankings:rankings lj 1!queries;
```
### 2.2 Creating an Embedding Function

Next, we'll set up a function to convert our text into vector embeddings. These embeddings are numerical representations of the text that capture its semantic meaning.

We'll use KDB-X Python to import PyTorch and the sentence-transformers library. For better performance, we'll also apply dynamic quantization to our model.
```q
torch:.pykx.import`torch;
ST:.pykx.import[`sentence_transformers;`:SentenceTransformer];
qd:.pykx.import[`torch.quantization;`:quantize_dynamic];
qconf:.pykx.import[`torch.quantization;`:default_qconfig];
torch[`:set_num_threads][3];
```
We load a lightweight and efficient transformer model, paraphrase-MiniLM-L3-v2. This model is great for creating high-quality embeddings quickly.
```q
model:ST[`$"paraphrase-MiniLM-L3-v2";`device pykw `cpu];
tqint8:torch[`:qint8];
qmodel:qd[model;.pykx.eval["lambda x: {x}"] torch[`:nn.Linear];`dtype pykw tqint8];
embed:{x[`:encode][.pykx.eval["lambda x: x.decode('utf-8') if type(x) is bytes else [x.decode('utf-8') for x in x]"] .pykx.topy y]`}[qmodel;];
```
We now have an embed function that can take a string and return a vector embedding:
```q
embed "Hello World!";
```

### 2.3 Creating a Tokenizing Function

For our sparse search, we need to break down our text into individual tokens (words or sub-words). We'll use a tokenizer from the transformers library for this.
```q
AT:.pykx.import[`transformers;`:AutoTokenizer];
tokenizer:AT[`$":from_pretrained"][`$"bert-base-uncased"];
tokenize:{if[10h~type y;y:`$y];x[$[":"~first string y;`$1_string y;y]][`$":input_ids"]`}[tokenizer;];
```
Our new tokenize function takes a string and returns a list of token IDs.
```q
tokenize[`$"abc abc abc"];
```

### 2.4 Updating the Dataset

Now we'll apply our embed and tokenize functions to our corpus and rankings tables, adding the new vector embeddings and token lists as new columns.
```q
rankings:update tokens:tokenize each text from rankings;
rankings:update embeddings:embed each text from rankings;

corpus:update tokens:tokenize each text from corpus;
corpus:update embeddings:embed text from corpus;
```

---

## 3. Performing the Search

With our data prepared, we can now perform our dense and sparse searches and then combine them for our hybrid result.

### 3.1 Sparse Search

For our sparse search, we'll use the BM25 algorithm, a powerful and widely-used ranking function for information retrieval.

The `.ai.bm25.put` function inserts sparse vectors into a bm25 index. The ck and cb parameters are hyperparameters that can be tuned to optimize the search results.

- k1 (ck): Controls how the term frequency of each word affects the relevance score (term saturation).
- b (cb): Controls how the length of a document affects the relevance score.

The `.ai.bm25.search` function returns the top `k` nearest neighbors for sparse search.

Finally to calculate accuracy, we find the intersection between the `sparse` search results and the ground truth `rankings` list. 
This tells us how many of the top documents we found were actually correct. Lastly, `avg` takes the average of all individual precision scores across all queries.
```q
ck:1.75e;
cb:0.25e;
index:.ai.bm25.put[()!();ck;cb;corpus[`tokens]];

sparse:corpus[`docId]@/:.[;(::;1)].ai.bm25.search[index;;10;ck;cb] each rankings[`tokens];
acc:avg (count each {inter[x;y]}'[sparse;rankings`rankedIds]);
0N!acc
```
**Result:** 0.66 (66% accuracy)

### 3.2 Dense Search

For our dense search, we'll use a flat (brute-force) search to find the nearest neighbors to our query embeddings in the corpus embeddings. We'll use the L2 distance metric (Euclidean distance) to measure similarity.
```q
dense:corpus[`docId]@/:.[;(::;1)].ai.flat.search[corpus[`embeddings];rankings[`embeddings];10;`L2];
acc:avg (count each {inter[x;y]}'[dense;rankings`rankedIds]);
0N!acc
```
**Result:** 0.64 (64% accuracy)

### 3.3 Hybrid Search

Finally, we combine the results of our sparse and dense searches using Reciprocal Rank Fusion (RRF). RRF is a method that combines multiple ranked lists into a single, more accurate list.

The `.ai.hybrid.rrf` function takes the two lists of search results and merges them. The `60` in this case is a constant that can be tuned.

```q
hybrid:10#/:{.ai.hybrid.rrf[(x;y);60]}'[sparse;dense];
acc:avg (count each {inter[x;y]}'[hybrid;rankings`rankedIds]);
0N!acc
```

**Result:** 0.7 (70% accuracy)

---

## Conclusion

As you can see, the hybrid search significantly outperforms both the sparse and dense searches on their own, demonstrating the power of this combined approach.
